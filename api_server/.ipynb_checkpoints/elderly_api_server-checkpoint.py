"""
è€å¹´äººå…³æ€€æ¨¡å‹ API æœåŠ¡å™¨
å…¼å®¹ OpenAI API æ ¼å¼ï¼Œå¯ç›´æ¥æ¥å…¥ Open WebUI
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import time
import argparse
import os

# ==================== æ•°æ®æ¨¡å‹å®šä¹‰ ====================

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "elderly-care"

# ==================== æ¨¡å‹ç®¡ç†ç±» ====================

class ElderlyModelAPI:
    def __init__(self, model_path: str, adapter_path: Optional[str] = None, model_type: str = "sft"):
        """
        åˆå§‹åŒ–è€å¹´äººå…³æ€€æ¨¡å‹API
        
        Args:
            model_path: åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–SFTåˆå¹¶åçš„æ¨¡å‹è·¯å¾„
            adapter_path: LoRAé€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            model_type: æ¨¡å‹ç±»å‹ï¼Œ'sft' æˆ– 'ppo'
        """
        self.model_path = model_path
        self.adapter_path = adapter_path
        self.model_type = model_type
        self.device = self._get_device()
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè€å¹´äººæä¾›ç”Ÿæ´»å¸®åŠ©å’Œå¥åº·å’¨è¯¢çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨æ¸©å’Œã€è€å¿ƒã€è¯¦ç»†çš„è¯­è¨€å›ç­”é—®é¢˜ï¼Œè€ƒè™‘åˆ°è€å¹´äººå¯èƒ½å­˜åœ¨çš„è§†åŠ›ã€å¬åŠ›å’Œè®¤çŸ¥èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚"
        
        print(f"ğŸš€ æ­£åœ¨åŠ è½½è€å¹´äººå…³æ€€æ¨¡å‹ ({model_type.upper()})...")
        self._load_model()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ä½¿ç”¨è®¾å¤‡: {self.device}")

    def _get_device(self):
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        return "cuda" if torch.cuda.is_available() else "cpu"

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, use_fast=False, trust_remote_code=True
        )
        
        # è®¾ç½®pad_tokenï¼Œé¿å…ä¸eos_tokenç›¸åŒå¯¼è‡´è­¦å‘Š
        if self.tokenizer.pad_token is None:
            if self.tokenizer.unk_token is not None:
                self.tokenizer.pad_token = self.tokenizer.unk_token
            else:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )

        # å¦‚æœæä¾›äº†adapterè·¯å¾„ï¼ŒåŠ è½½LoRAé€‚é…å™¨
        if self.adapter_path and os.path.exists(self.adapter_path):
            print(f"ğŸ“¦ åŠ è½½LoRAé€‚é…å™¨: {self.adapter_path}")
            self.model = PeftModel.from_pretrained(self.model, model_id=self.adapter_path)
        
        self.model.eval()

    def generate_response(
        self, 
        messages: List[Dict[str, str]], 
        temperature: float = 0.7, 
        top_p: float = 0.9, 
        max_tokens: int = 512
    ) -> str:
        """
        ç”Ÿæˆå›å¤
        
        Args:
            messages: å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}, ...]
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ·»åŠ system promptï¼‰
        full_messages = []
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯
        has_system = any(msg.get("role") == "system" for msg in messages)
        if not has_system:
            full_messages.append({"role": "system", "content": self.system_prompt})
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        full_messages.extend(messages)
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = self.tokenizer.apply_chat_template(
            full_messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # Tokenizeè¾“å…¥
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        # è§£ç è¾“å‡º
        response_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
        
        return response.strip()

# ==================== FastAPIåº”ç”¨ ====================

app = FastAPI(
    title="è€å¹´äººå…³æ€€æ¨¡å‹API",
    description="åŸºäºQwen3å¾®è°ƒçš„è€å¹´äººå…³æ€€åŠ©æ‰‹ï¼Œå…¼å®¹OpenAI APIæ ¼å¼",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶ï¼Œå…è®¸Open WebUIè®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®æŒ‡å®šå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨¡å‹å®ä¾‹
model_api: Optional[ElderlyModelAPI] = None

# ==================== APIç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›APIä¿¡æ¯"""
    return {
        "message": "è€å¹´äººå…³æ€€æ¨¡å‹APIæœåŠ¡",
        "version": "1.0.0",
        "status": "running",
        "model_loaded": model_api is not None
    }

@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹ï¼ˆOpenAI APIå…¼å®¹ï¼‰"""
    model_id = f"elderly-care-{model_api.model_type}" if model_api else "elderly-care"
    return {
        "object": "list",
        "data": [
            {
                "id": model_id,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "elderly-care",
                "permission": [],
                "root": model_id,
                "parent": None,
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    èŠå¤©è¡¥å…¨ç«¯ç‚¹ï¼ˆOpenAI APIå…¼å®¹ï¼‰
    è¿™æ˜¯Open WebUIè°ƒç”¨çš„ä¸»è¦æ¥å£
    """
    if model_api is None:
        raise HTTPException(status_code=500, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        
        # ç”Ÿæˆå›å¤
        response_text = model_api.generate_response(
            messages=messages,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens
        )
        
        # æ„å»ºå“åº”ï¼ˆOpenAIæ ¼å¼ï¼‰
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": 0,  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸è®¡ç®—token
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "model_loaded": model_api is not None,
        "device": model_api.device if model_api else "unknown"
    }

# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="è€å¹´äººå…³æ€€æ¨¡å‹APIæœåŠ¡å™¨")
    parser.add_argument(
        "--model_path", 
        type=str, 
        required=True, 
        help="åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–SFTåˆå¹¶åçš„æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--adapter_path", 
        type=str, 
        default=None, 
        help="LoRAé€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
    )
    parser.add_argument(
        "--model_type", 
        type=str, 
        default="sft", 
        choices=["sft", "ppo"],
        help="æ¨¡å‹ç±»å‹: sft æˆ– ppo"
    )
    parser.add_argument(
        "--host", 
        type=str, 
        default="0.0.0.0", 
        help="æœåŠ¡å™¨ç›‘å¬åœ°å€"
    )
    parser.add_argument(
        "--port", 
        type=int, 
        default=8000, 
        help="æœåŠ¡å™¨ç«¯å£"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return
    
    if args.adapter_path and not os.path.exists(args.adapter_path):
        print(f"âŒ é”™è¯¯: é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
        return
    
    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global model_api
    model_api = ElderlyModelAPI(
        model_path=args.model_path,
        adapter_path=args.adapter_path,
        model_type=args.model_type
    )
    
    print("\n" + "="*80)
    print("ğŸ‰ è€å¹´äººå…³æ€€æ¨¡å‹APIæœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼")
    print("="*80)
    print(f"ğŸ“¡ APIåœ°å€: http://{args.host}:{args.port}")
    print(f"ğŸ“š APIæ–‡æ¡£: http://{args.host}:{args.port}/docs")
    print(f"ğŸ”— OpenAIå…¼å®¹ç«¯ç‚¹: http://{args.host}:{args.port}/v1/chat/completions")
    print("\nğŸ’¡ åœ¨Open WebUIä¸­é…ç½®:")
    print(f"   - API URL: http://localhost:{args.port}/v1")
    print(f"   - API Key: éšæ„å¡«å†™ï¼ˆæœ¬æœåŠ¡ä¸éªŒè¯ï¼‰")
    print(f"   - æ¨¡å‹åç§°: elderly-care-{args.model_type}")
    print("="*80 + "\n")
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(app, host=args.host, port=args.port)

if __name__ == "__main__":
    main()
