import json
import os
import torch
from datasets import Dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import LoraConfig, get_peft_model
from trl import RewardConfig, RewardTrainer
import swanlab
from dataclasses import dataclass, field
from transformers import HfArgumentParser

@dataclass
class ScriptArguments:
    model_path: str = field(metadata={"help": "SFTåˆå¹¶åçš„æ¨¡å‹è·¯å¾„"})
    dataset_path: str = field(default="data/mixed_partially_shuffled.json", metadata={"help": "æ•°æ®é›†è·¯å¾„"})
    rm_adapter_output_dir: str = field(default="./output/elderly/rm_adapter_elderly", metadata={"help": "RM LoRAé€‚é…å™¨ä¿å­˜ç›®å½•"})
    system_prompt: str = field(default="ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè€å¹´äººæä¾›ç”Ÿæ´»å¸®åŠ©å’Œå¥åº·å’¨è¯¢çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨æ¸©å’Œã€è€å¿ƒã€è¯¦ç»†çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚", metadata={"help": "ç³»ç»Ÿæç¤ºè¯­"})
    max_length: int = field(default=1024, metadata={"help": "è¾“å…¥æœ€å¤§é•¿åº¦"})
    lora_r: int = field(default=8, metadata={"help": "LoRAçš„ç§©"})
    lora_alpha: int = field(default=16, metadata={"help": "LoRAçš„alpha"})
    lora_dropout: float = field(default=0.1, metadata={"help": "LoRAçš„dropout"})
    use_swanlab: bool = field(default=True, metadata={"help": "æ˜¯å¦ä½¿ç”¨SwanLab"})

def setup_swanlab(args: ScriptArguments):
    if not args.use_swanlab:
        return
    os.environ["SWANLAB_PROJECT"] = "qwen3-sft-rm-elderly"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    swanlab.init(
        project="qwen3-sft-rm-elderly",
        run_name="rm-training-elderly",
        config=vars(args)
    )

def load_rm_dataset(dataset_path):
    """åŠ è½½è€å¹´äººæ•°æ®é›†å¹¶è½¬æ¢ä¸ºRMæ ¼å¼"""
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒé”™è¯¯: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ° at {dataset_path}")
        exit()
    
    processed_data = []
    for item in data:
        # ä»æ–°æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸ºRMæ ¼å¼
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output_text = item.get('output', '')
        system_text = item.get('system', '')
        
        # æ„é€ human input
        if input_text:
            human_input = f"{instruction}\n\n{input_text}" if instruction else input_text
        else:
            human_input = instruction if instruction else system_text
            
        # å¯¹äºRMè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦æ„é€ chosenå’Œrejectedæ ·æœ¬
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹è¾“å‡ºä½œä¸ºchosenï¼Œé€šè¿‡æ·»åŠ å™ªå£°æˆ–ä¿®æ”¹æ¥åˆ›å»ºrejectedæ ·æœ¬
        chosen_response = output_text
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„rejectedç‰ˆæœ¬ï¼ˆé€šè¿‡æˆªæ–­è¾“å‡ºï¼‰
        if len(output_text) > 50:
            rejected_response = output_text[:len(output_text)//2]  # å–å‰åŠéƒ¨åˆ†ä½œä¸ºrejected
        else:
            rejected_response = output_text[:-10] if len(output_text) > 10 else output_text  # å»æ‰æœ€åå‡ ä¸ªå­—ç¬¦
            
        # ç¡®ä¿rejectedå’Œchosenä¸ç›¸åŒ
        if rejected_response == chosen_response:
            rejected_response = chosen_response + " "  # æ·»åŠ ç©ºæ ¼ä½¿å…¶ä¸åŒ

        if human_input and chosen_response and rejected_response:
            processed_data.append({
                "input": human_input,
                "chosen": chosen_response,
                "rejected": rejected_response
            })
    return processed_data

def main():
    parser = HfArgumentParser(ScriptArguments)
    args = parser.parse_args_into_dataclasses()[0]

    if not os.path.exists(args.model_path):
        print(f"âŒé”™è¯¯: åŸºç¡€æ¨¡å‹ (SFTåˆå¹¶å) åœ¨ '{args.model_path}' æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡ŒSFTå¾®è°ƒå’Œåˆå¹¶è„šæœ¬ã€‚")
        exit()

    print("ğŸš€ 1. é…ç½®å’Œåˆå§‹åŒ– SwanLab...")
    setup_swanlab(args)
    
    print("ğŸš€ 2. åŠ è½½Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_path, use_fast=False, trust_remote_code=True, padding_side="left"
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    def preprocess_function(examples):
        new_examples = {"input_ids_chosen": [], "attention_mask_chosen": [], "input_ids_rejected": [], "attention_mask_rejected": []}
        for human_input, chosen, rejected in zip(examples["input"], examples["chosen"], examples["rejected"]):
            text_chosen = tokenizer.apply_chat_template(
                [{"role": "system", "content": args.system_prompt}, {"role": "user", "content": human_input}, {"role": "assistant", "content": chosen}],
                tokenize=False, add_generation_prompt=False
            )
            text_rejected = tokenizer.apply_chat_template(
                [{"role": "system", "content": args.system_prompt}, {"role": "user", "content": human_input}, {"role": "assistant", "content": rejected}],
                tokenize=False, add_generation_prompt=False
            )
            tokenized_chosen = tokenizer(text_chosen, truncation=True, max_length=args.max_length)
            tokenized_rejected = tokenizer(text_rejected, truncation=True, max_length=args.max_length)
            
            new_examples["input_ids_chosen"].append(tokenized_chosen["input_ids"])
            new_examples["attention_mask_chosen"].append(tokenized_chosen["attention_mask"])
            new_examples["input_ids_rejected"].append(tokenized_rejected["input_ids"])
            new_examples["attention_mask_rejected"].append(tokenized_rejected["attention_mask"])
        return new_examples

    print("ğŸš€ 3. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†...")
    raw_data = load_rm_dataset(args.dataset_path)
    full_dataset = Dataset.from_list(raw_data)
    train_test_split = full_dataset.train_test_split(test_size=0.1)
    train_dataset = train_test_split['train'].map(preprocess_function, batched=True)
    eval_dataset = train_test_split['test'].map(preprocess_function, batched=True)
    print(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(eval_dataset)}")
    
    print("ğŸš€ 4. åŠ è½½æ¨¡å‹å¹¶é…ç½®LoRA for RM...")
    model = AutoModelForSequenceClassification.from_pretrained(
        args.model_path, num_labels=1, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True
    )
    model.config.use_cache = False
    
    rm_lora_config = LoraConfig(
        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        bias="none", task_type="SEQ_CLS",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )
    model = get_peft_model(model, rm_lora_config)
    print("å·²ä¸ºRMä»»åŠ¡æ·»åŠ æ–°çš„å¯è®­ç»ƒLoRAé€‚é…å™¨ã€‚")
    model.print_trainable_parameters()

    print("ğŸš€ 5. é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = RewardConfig(
        output_dir="./output/elderly/rm_model_temp_elderly",
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,
        num_train_epochs=1,
        logging_steps=10,
        eval_strategy="steps", eval_steps=100,
        save_strategy="steps", save_steps=200, save_total_limit=2,
        gradient_checkpointing=True,
        remove_unused_columns=False,
        report_to="swanlab" if args.use_swanlab else "none",
        run_name="rm-training-elderly",
        lr_scheduler_type="cosine",
        warmup_steps=50,
        max_length=args.max_length,
    )

    print("ğŸš€ 6. åˆ›å»ºå¹¶å¯åŠ¨RewardTrainer...")
    trainer = RewardTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer,
    )
    trainer.train()

    print(f"ğŸ’¾ 7. ä¿å­˜RM LoRAé€‚é…å™¨åˆ°: {args.rm_adapter_output_dir}")
    os.makedirs(args.rm_adapter_output_dir, exist_ok=True)
    trainer.save_model(args.rm_adapter_output_dir)
    
    print("âœ… å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    if args.use_swanlab:
        swanlab.finish()

if __name__ == "__main__":
    main()