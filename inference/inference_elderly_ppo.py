import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import argparse
import os

class ElderlyPPOChatbot:
    def __init__(self, sft_merged_model_path, ppo_adapter_path):
        self.device = self._get_device()
        self.sft_merged_model_path = sft_merged_model_path
        self.ppo_adapter_path = ppo_adapter_path
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè€å¹´äººæä¾›ç”Ÿæ´»å¸®åŠ©å’Œå¥åº·å’¨è¯¢çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨æ¸©å’Œã€è€å¿ƒã€è¯¦ç»†çš„è¯­è¨€å›ç­”é—®é¢˜ï¼Œè€ƒè™‘åˆ°è€å¹´äººå¯èƒ½å­˜åœ¨çš„è§†åŠ›ã€å¬åŠ›å’Œè®¤çŸ¥èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚"
        self.tokenizer = None
        self.model = None

    def _get_device(self):
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        return "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """åŠ è½½PPOå¾®è°ƒåçš„æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åŠ è½½è€å¹´äººå…³æ€€PPOæ¨¡å‹...")
        print(f"--> åŸºç¡€æ¨¡å‹ (SFTåˆå¹¶å): {self.sft_merged_model_path}")
        print(f"--> PPOé€‚é…å™¨: {self.ppo_adapter_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.sft_merged_model_path, use_fast=False, trust_remote_code=True
        )
        # è®¾ç½®pad_tokenï¼Œé¿å…ä¸eos_tokenç›¸åŒå¯¼è‡´è­¦å‘Š
        if self.tokenizer.pad_token is None:
            # ä¼˜å…ˆä½¿ç”¨unk_tokenï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨eos_token
            if self.tokenizer.unk_token is not None:
                self.tokenizer.pad_token = self.tokenizer.unk_token
            else:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                # å¦‚æœpad_tokenå’Œeos_tokenç›¸åŒï¼Œè®¾ç½®ä¸åŒçš„pad_token_id
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        self.model = AutoModelForCausalLM.from_pretrained(
            self.sft_merged_model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )

        self.model = PeftModel.from_pretrained(
            self.model, model_id=self.ppo_adapter_path
        )
        self.model.eval()
        print(f"âœ… PPOæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.model.device}")

    def generate_response(self, prompt, max_new_tokens=512, temperature=0.7, top_p=0.9):
        """ä½¿ç”¨èŠå¤©æ¨¡æ¿ç”Ÿæˆå›å¤"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,  # æ˜¾å¼ä¼ é€’attention_mask
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
        return response.strip()

def test_model(chatbot: ElderlyPPOChatbot, output_file: str):
    """æ‰¹é‡æµ‹è¯•PPOæ¨¡å‹æ•ˆæœ"""
    test_questions = [
        "æˆ‘æœ€è¿‘æ€»æ˜¯å¿˜è®°äº‹æƒ…ï¼Œè¿™æ˜¯è€å¹´ç—´å‘†å—ï¼Ÿ",
        "å¦‚ä½•ä¿æŒèº«ä½“å¥åº·ï¼Ÿ",
        "è€å¹´äººåº”è¯¥æ€æ ·åˆç†é¥®é£Ÿï¼Ÿ",
        "æˆ‘æ™šä¸Šæ€»æ˜¯ç¡ä¸å¥½ï¼Œæœ‰ä»€ä¹ˆåŠæ³•å—ï¼Ÿ",
        "å¦‚ä½•é¢„é˜²è·Œå€’ï¼Ÿ",
        "è€å¹´äººéœ€è¦è¡¥å……å“ªäº›ç»´ç”Ÿç´ ï¼Ÿ",
        "å¦‚ä½•ä¿æŒå¿ƒæƒ…æ„‰å¿«ï¼Ÿ",
        "è€å¹´äººé€‚åˆåšä»€ä¹ˆè¿åŠ¨ï¼Ÿ",
        "æˆ‘çš„è¡€å‹æœ‰ç‚¹é«˜ï¼Œåº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ä½¿ç”¨æ™ºèƒ½æ‰‹æœºæ‹ç…§ï¼Ÿ"
    ]

    print("\n" + "="*80 + "\nğŸ¯ è€å¹´äººå…³æ€€PPOæ¨¡å‹æ‰¹é‡æµ‹è¯•å¼€å§‹\n" + "="*80)
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_questions)}: {question}\n" + "-" * 60)
        response = chatbot.generate_response(question)
        print(f"ğŸ¤– å›å¤: {response}")
        results.append({"question": question, "response": response})
        print("-" * 60)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… PPOæµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

def interactive_chat(chatbot: ElderlyPPOChatbot):
    """PPOæ¨¡å‹äº¤äº’å¼å¯¹è¯"""
    print("\n" + "="*80 + "\nğŸ¯ è€å¹´äººå…³æ€€åŠ©æ‰‹äº¤äº’å¼å¯¹è¯ (PPOå¢å¼ºç‰ˆ)\n" + "="*80)
    print("ğŸ’¡ è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")

    while True:
        try:
            # ä½¿ç”¨æ›´å¥å£®çš„è¾“å…¥æ–¹å¼ï¼Œå¤„ç†ç¼–ç é—®é¢˜
            try:
                user_input = input("\nğŸ‘´ è€å¹´äºº: ").strip()
            except UnicodeDecodeError:
                # å¦‚æœç›´æ¥inputå‡ºç°é—®é¢˜ï¼Œå°è¯•å…¶ä»–æ–¹å¼
                import sys
                line = sys.stdin.readline()
                user_input = line.strip() if line else ""

            if user_input.lower() in ['exit', 'quit']: break
            if not user_input: continue
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            response = chatbot.generate_response(user_input)
            print(response)
        except UnicodeDecodeError as e:
            print(f"\nâŒ è¾“å…¥ç¼–ç é”™è¯¯: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿è¾“å…¥çš„æ˜¯æœ‰æ•ˆçš„UTF-8ç¼–ç æ–‡æœ¬")
            continue
        except (KeyboardInterrupt, EOFError):
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            print("ğŸ’¡ è¯·é‡æ–°è¾“å…¥")
            continue
    print("\nğŸ‘‹ å†è§ï¼ç¥æ‚¨èº«ä½“å¥åº·ï¼")

def main():
    parser = argparse.ArgumentParser(description="è€å¹´äººå…³æ€€PPOæ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="SFTåˆå¹¶åçš„åŸºç¡€æ¨¡å‹çš„è·¯å¾„ (ä¾‹å¦‚ ./output/elderly/sft_merged_model)")
    parser.add_argument("--adapter_path", type=str, required=True, help="PPO LoRAé€‚é…å™¨çš„è·¯å¾„ (ä¾‹å¦‚ ./output/elderly/ppo_adapter)")
    parser.add_argument("--mode", type=str, default="interactive", choices=["interactive", "test"], help="è¿è¡Œæ¨¡å¼: 'interactive' (äº¤äº’å¼) æˆ– 'test' (æ‰¹é‡æµ‹è¯•)")
    parser.add_argument("--test_output_file", type=str, default="elderly_ppo_test_results.json", help="æ‰¹é‡æµ‹è¯•ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        print(f"âŒé”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return
    if not os.path.exists(args.adapter_path):
        print(f"âŒé”™è¯¯: é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
        return

    chatbot = ElderlyPPOChatbot(args.model_path, args.adapter_path)
    chatbot.load_model()

    if args.mode == 'interactive':
        interactive_chat(chatbot)
    elif args.mode == 'test':
        test_model(chatbot, args.test_output_file)

if __name__ == "__main__":
    main()
