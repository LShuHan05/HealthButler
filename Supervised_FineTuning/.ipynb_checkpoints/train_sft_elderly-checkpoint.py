import json
import os
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    HfArgumentParser
)
from peft import LoraConfig, get_peft_model
import swanlab
from dataclasses import dataclass, field
from typing import Optional

@dataclass
class ScriptArguments:
    """
    SFTè„šæœ¬çš„é…ç½®å‚æ•°
    """
    model_path: str = field(metadata={"help": "æ¨¡å‹ä»“åº“çš„è·¯å¾„"})
    dataset_path: str = field(default="data/mixed_partially_shuffled.json", metadata={"help": "æ•°æ®é›†çš„è·¯å¾„"})
    sft_adapter_output_dir: str = field(default="./output/sft_adapter_elderly", metadata={"help": "SFT LoRAé€‚é…å™¨çš„ä¿å­˜ç›®å½•"})
    system_prompt: str = field(default="ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè€å¹´äººæä¾›ç”Ÿæ´»å¸®åŠ©å’Œå¥åº·å’¨è¯¢çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨æ¸©å’Œã€è€å¿ƒã€è¯¦ç»†çš„è¯­è¨€å›ç­”é—®é¢˜ï¼Œè€ƒè™‘åˆ°è€å¹´äººå¯èƒ½å­˜åœ¨çš„è§†åŠ›ã€å¬åŠ›å’Œè®¤çŸ¥èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚", metadata={"help": "ç³»ç»Ÿæç¤ºè¯­"})
    max_length: int = field(default=1024, metadata={"help": "è¾“å…¥çš„æœ€å¤§é•¿åº¦"})
    lora_r: int = field(default=8, metadata={"help": "LoRAçš„ç§©"})
    lora_alpha: int = field(default=16, metadata={"help": "LoRAçš„alpha"})
    lora_dropout: float = field(default=0.1, metadata={"help": "LoRAçš„dropout"})
    use_swanlab: bool = field(default=True, metadata={"help": "æ˜¯å¦ä½¿ç”¨SwanLabè®°å½•å®éªŒ"})

def setup_swanlab(args: ScriptArguments):
    """é…ç½®å¹¶åˆå§‹åŒ–SwanLab"""
    if not args.use_swanlab:
        return
    
    os.environ["SWANLAB_PROJECT"] = "qwen3-sft-elderly"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    swanlab.init(
        project="qwen3-sft-elderly",
        run_name="sft-training-elderly",
        config={
            "model": args.model_path,
            "method": "SFT_with_Trainer",
            "lora_r": args.lora_r,
            "lora_alpha": args.lora_alpha,
            "dataset": args.dataset_path,
            "system_prompt": args.system_prompt
        }
    )

def load_and_format_dataset(dataset_path, system_prompt):
    """
    åŠ è½½è€å¹´äººå…³æ€€JSONæ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºSFTçš„ instruction, input, output æ ¼å¼.
    """
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒé”™è¯¯: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ° at {dataset_path}")
        exit()
    
    formatted_data = []
    for item in data:
        # æ–°æ•°æ®é›†æ ¼å¼ç›´æ¥åŒ…å« instruction, input, output å­—æ®µ
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output_text = item.get('output', '')
        system_text = item.get('system', system_prompt)  # å¦‚æœæ•°æ®ä¸­æœ‰systemå­—æ®µåˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„instructionï¼Œä½¿ç”¨é»˜è®¤çš„ç³»ç»Ÿæç¤ºè¯­
        if not instruction:
            instruction = system_text
            
        if input_text and output_text:
            formatted_data.append({
                "instruction": instruction,
                "input": input_text,
                "output": output_text
            })
        elif instruction and output_text:  # æœ‰äº›æ•°æ®å¯èƒ½æ²¡æœ‰inputå­—æ®µ
            formatted_data.append({
                "instruction": instruction,
                "input": "",
                "output": output_text
            })
    return formatted_data

def main():
    parser = HfArgumentParser(ScriptArguments)
    args = parser.parse_args_into_dataclasses()[0]

    print("ğŸš€ 1. é…ç½®å’Œåˆå§‹åŒ– SwanLab...")
    setup_swanlab(args)

    print("ğŸš€ 2. åŠ è½½å’Œæ ¼å¼åŒ–æ•°æ®é›†...")
    sft_data = load_and_format_dataset(args.dataset_path, args.system_prompt)
    full_dataset = Dataset.from_list(sft_data)
    
    # ä½¿ç”¨ train_test_split åˆ’åˆ†æ•°æ®é›†
    train_test_split = full_dataset.train_test_split(test_size=0.1)
    train_dataset = train_test_split['train']
    eval_dataset = train_test_split['test']
    print(f"SFTè®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(eval_dataset)}")

    print("ğŸš€ 3. åŠ è½½Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_path, 
        use_fast=False, 
        trust_remote_code=True,
        padding_side="right"
    )
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    def process_func(example):
        # æ ¹æ®æ–°æ•°æ®é›†çš„ç‰¹ç‚¹è°ƒæ•´æ ¼å¼
        if example['input']:
            full_input = f"{example['instruction']}\n\n{example['input']}"
        else:
            full_input = example['instruction']
            
        # æ„é€ èŠå¤©æ¨¡æ¿æ ¼å¼çš„è¾“å…¥
        chat = [
            {"role": "system", "content": example['instruction']},
            {"role": "user", "content": example['input']},
            {"role": "assistant", "content": example['output']}
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        prompt = tokenizer.apply_chat_template(chat[:-1], tokenize=False, add_generation_prompt=True)
        full_text = prompt + example['output'] + tokenizer.eos_token
        
        # Tokenizeå®Œæ•´æ–‡æœ¬
        model_inputs = tokenizer(full_text, max_length=args.max_length, truncation=True)
        
        # åˆ›å»ºlabelsï¼ˆåªå¯¹assistantçš„å›å¤è®¡ç®—æŸå¤±ï¼‰
        prompt_ids = tokenizer(prompt, add_special_tokens=False)['input_ids']
        labels = [-100] * len(prompt_ids) + model_inputs["input_ids"][len(prompt_ids):]
        
        # ç¡®ä¿labelsä¸ä¼šè¶…è¿‡æœ€å¤§é•¿åº¦
        if len(labels) > args.max_length:
            labels = labels[:args.max_length]
            
        model_inputs["labels"] = labels
        return model_inputs

    print("ğŸš€ 4. å¯¹æ•°æ®é›†è¿›è¡ŒTokenization...")
    tokenized_train_ds = train_dataset.map(process_func, remove_columns=train_dataset.column_names)
    tokenized_eval_ds = eval_dataset.map(process_func, remove_columns=eval_dataset.column_names)
    
    print("ğŸš€ 5. åŠ è½½æ¨¡å‹å¹¶é…ç½®LoRA...")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model.enable_input_require_grads()
    model.config.use_cache = False

    lora_config = LoraConfig(
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    print("ğŸš€ 6. é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir="./output/elderly/sft_model_temp_elderly",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=1e-4,
        num_train_epochs=1,
        logging_steps=10,
        save_strategy="epoch",
        eval_strategy="epoch",
        gradient_checkpointing=True,
        report_to="swanlab" if args.use_swanlab else "none",
        run_name="sft-training-elderly",
    )

    print("ğŸš€ 7. åˆ›å»ºå¹¶å¯åŠ¨Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_ds,
        eval_dataset=tokenized_eval_ds,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    )
    trainer.train()

    print(f"ğŸ’¾ 8. ä¿å­˜SFT LoRAé€‚é…å™¨åˆ°: {args.sft_adapter_output_dir}")
    os.makedirs(args.sft_adapter_output_dir, exist_ok=True)
    trainer.save_model(args.sft_adapter_output_dir)
    
    print("\nâœ… SFTè®­ç»ƒå®Œæˆï¼")
    if args.use_swanlab:
        swanlab.finish()

if __name__ == "__main__":
    main()